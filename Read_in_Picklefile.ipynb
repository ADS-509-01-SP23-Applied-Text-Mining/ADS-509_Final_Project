{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.2: Preparing Data for Final Team Project\n",
    "\n",
    "Instructions:\n",
    "1. Set up a GitHub repository for your final project. Make it public, or add your instructor as a collaborator. Include in this repository the code to assemble your final project data set. The best practice is to place this data creation code in its own notebook.\n",
    "2. In a separate notebook, calculate descriptive statistics on your final-project data. You are welcome to reuse code from earlier modules.\n",
    "\n",
    "Deliverable:\n",
    "* When you have completed this notebook, run all cells, print the notebook as a PDF, and submit the PDF in Blackboard.\n",
    "* Commit and push your code, so your repo is up to date.\n",
    "* Enter your GitHub link as “online text” in the Blackboard assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reinforcement Learning from Multiple Sensors v...</td>\n",
       "      <td>In many scenarios, observations from more than...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interventional Causal Representation Learning</td>\n",
       "      <td>Causal representation learning seeks to extrac...</td>\n",
       "      <td>[stat.ML, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-Supervised Node Representation Learning v...</td>\n",
       "      <td>Self-supervised node representation learning a...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Out-of-Distribution Representation Learning fo...</td>\n",
       "      <td>Time series classification is an important pro...</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trading Information between Latents in Hierarc...</td>\n",
       "      <td>Variational Autoencoders (VAEs) were originall...</td>\n",
       "      <td>[stat.ML, cs.CV, cs.IT, cs.LG, math.IT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64568</th>\n",
       "      <td>Plot 94 in ambiance X-Window</td>\n",
       "      <td>&lt;PLOT &gt; is a collection of routines to draw su...</td>\n",
       "      <td>[cs.CV, cs.GR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64569</th>\n",
       "      <td>Automatic Face Recognition System Based on Loc...</td>\n",
       "      <td>We present an automatic face verification syst...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64570</th>\n",
       "      <td>Convexity Analysis of Snake Models Based on Ha...</td>\n",
       "      <td>This paper presents a convexity analysis for t...</td>\n",
       "      <td>[cs.CV, cs.GR, I.4; I.4.6;I.4.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64571</th>\n",
       "      <td>Semi-automatic vectorization of linear network...</td>\n",
       "      <td>A system for semi-automatic vectorization of l...</td>\n",
       "      <td>[cs.CV, cs.MM, I.4.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64572</th>\n",
       "      <td>Digital Color Imaging</td>\n",
       "      <td>This paper surveys current technology and rese...</td>\n",
       "      <td>[cs.CV, cs.GR, A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64573 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles  \\\n",
       "0      Reinforcement Learning from Multiple Sensors v...   \n",
       "1          Interventional Causal Representation Learning   \n",
       "2      Self-Supervised Node Representation Learning v...   \n",
       "3      Out-of-Distribution Representation Learning fo...   \n",
       "4      Trading Information between Latents in Hierarc...   \n",
       "...                                                  ...   \n",
       "64568                       Plot 94 in ambiance X-Window   \n",
       "64569  Automatic Face Recognition System Based on Loc...   \n",
       "64570  Convexity Analysis of Snake Models Based on Ha...   \n",
       "64571  Semi-automatic vectorization of linear network...   \n",
       "64572                              Digital Color Imaging   \n",
       "\n",
       "                                               abstracts  \\\n",
       "0      In many scenarios, observations from more than...   \n",
       "1      Causal representation learning seeks to extrac...   \n",
       "2      Self-supervised node representation learning a...   \n",
       "3      Time series classification is an important pro...   \n",
       "4      Variational Autoencoders (VAEs) were originall...   \n",
       "...                                                  ...   \n",
       "64568  <PLOT > is a collection of routines to draw su...   \n",
       "64569  We present an automatic face verification syst...   \n",
       "64570  This paper presents a convexity analysis for t...   \n",
       "64571  A system for semi-automatic vectorization of l...   \n",
       "64572  This paper surveys current technology and rese...   \n",
       "\n",
       "                                                  terms  \n",
       "0                                               [cs.LG]  \n",
       "1                                      [stat.ML, cs.LG]  \n",
       "2                                               [cs.LG]  \n",
       "3                                        [cs.LG, cs.AI]  \n",
       "4               [stat.ML, cs.CV, cs.IT, cs.LG, math.IT]  \n",
       "...                                                 ...  \n",
       "64568                                    [cs.CV, cs.GR]  \n",
       "64569                                           [cs.CV]  \n",
       "64570                  [cs.CV, cs.GR, I.4; I.4.6;I.4.8]  \n",
       "64571                             [cs.CV, cs.MM, I.4.6]  \n",
       "64572  [cs.CV, cs.GR, A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2]  \n",
       "\n",
       "[64573 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Imported Data\n",
    "raw_data = pd.read_pickle('G:\\\\My Drive\\\\ADS-509_Final_Team_Project\\\\arxiv_data_2023_02_13.pkl')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function - Descriptive Statistics\n",
    "def descriptive_stats(tokens, top_n_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = ld.ttr(tokens) # Simple TTR = len(Counter(text))/len(text)\n",
    "    num_characters = sum([len(i) for i in tokens])\n",
    "    \n",
    "    if verbose:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"The top {top_n_tokens} most common tokens\")\n",
    "        print(Counter(tokens).most_common(top_n_tokens))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions - Cleaning data\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    \"\"\"This function removes stopwords from a list of tokens.\"\"\"\n",
    "    return([t for t in tokens if t.lower() not in sw])\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    \"\"\"This function removes punctuation from a string.\"\"\"\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    return text.lower().strip().split() \n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    \"\"\" This fuction manages and executes other functions like a pipline. \"\"\"\n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function - Flatten a List of Lists\n",
    "def flatten_and_descriptive_stats(list_of_lists):\n",
    "    \"\"\"This function flattens a list of lists into a single list.\"\"\"\n",
    "    return [i for s in list_of_lists for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Data Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Plot our Work Charts by Category."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function - Plot Word Cloud\n",
    "\n",
    "\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "def plot_wc(wordcloud_df):\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)###\n",
    "    wordcloud(wordcloud_df['freq'], max_words=1000)\n",
    "    plt.subplot(1,2,2)###\n",
    "    wordcloud(wordcloud_df['freq'], max_words=1000, stopwords=sw)\n",
    "    plt.tight_layout()###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_df = count_words(lyrics_data.loc[lyrics_data['artist']=='cher'])\n",
    "plot_wc(wordcloud_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b33cfd37f5195bd7836c1451c6eaacc84fbbad3c54541ec8bad2790bfb3f777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
